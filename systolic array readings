Systolic Arrays: Specialized Accelerators.
---------------------------------------------------------
Goal: 
  Design an accelerator that has simple, regular design, high concurrency, balanced computation, balancing I/O memory bandwidth. 
  Maximize data reuse and parallel computation, minimize memory access.
Idea: Replace single PE with an array of PE
Helps: To maximize coputation done on a single piece of data.

Difference from pipelinine ?
These have individual PEs.
Array structure can be non linear and multidimensional
PE connections can be multidirectional
PE can have local memory and execute kernels

Lets' suppose, you have to inputs A and B having 3x3 matrix data and you want to multiply that.
1 way of doing this is keep final result in PE.
2 way of doing is move the final result somewhere else.

https://www.youtube.com/watch?v=LecZfREuWvk&list=PL5Q2soXY2Zi_ZMtqz1r-GHm-zzuE1QfIg&index=9
Time - 38:37, see  explanation of matrix multiplication in a systolic array.

Advantages:
Principle followed: efficiently used memory bandwidth.
Specialized computation happening in PE.
Improved efficiency, high concurrency, simple design
Good to do more with less memory bandwidth.

Downside:
It's Specialized: Not generally applicable, because computation needs to fit the PE function organization.

More info:
Each PE can have its own data and Instruction memory
Data memory to store partial and temporary results
This leads to stream processing, pipeline parallelism
More generally staged execution

Papers:
Google VPU 2021 paper
Google TPU paper ISCA 2017 (MOST CITED PAPER IN HISTORY)



Example: Matrix Multiplication on Systolic Array

Letâ€™s compute C = A Ã— B, where:
	â€¢	A is MÃ—N
	â€¢	B is NÃ—P
	â€¢	Result C is MÃ—P

PE Operation

Each PE at position (i,j) computes:
C_{i,j} = \sum_{k=0}^{N-1} A_{i,k} Ã— B_{k,j}

â¸»

Dataflow
	1.	Elements of A enter from the left (one row per cycle).
	2.	Elements of B enter from the top (one column per cycle).
	3.	Each PE:
	â€¢	Multiplies inputs A_{i,k} and B_{k,j},
	â€¢	Adds to the running sum,
	â€¢	Passes A to the next PE in the row, B down the column.
	4.	After N cycles, every PE holds a final partial sum for one element of C.
3. Example: Matrix Multiplication on Systolic Array

Letâ€™s compute C = A Ã— B, where:
	â€¢	A is MÃ—N
	â€¢	B is NÃ—P
	â€¢	Result C is MÃ—P

PE Operation

Each PE at position (i,j) computes:
C_{i,j} = \sum_{k=0}^{N-1} A_{i,k} Ã— B_{k,j}

â¸»

Dataflow
	1.	Elements of A enter from the left (one row per cycle).
	2.	Elements of B enter from the top (one column per cycle).
	3.	Each PE:
	â€¢	Multiplies inputs A_{i,k} and B_{k,j},
	â€¢	Adds to the running sum,
	â€¢	Passes A to the next PE in the row, B down the column.
	4.	After N cycles, every PE holds a final partial sum for one element of C.

âœ… In essence:
A systolic array is a fixed-function dataflow engine â€” unbeatable for linear algebra (especially matrix multiplication).



ğŸ’¡ 6. Real-World Example â€” Google TPU

Google TPU v1 (2015)
	â€¢	256Ã—256 systolic array
	â€¢	Each PE = 1 MAC (Multiplyâ€“Accumulate)
	â€¢	Total = 65,536 operations per cycle
	â€¢	Clocked at ~700 MHz â†’ 46 TFLOPS (8-bit integer)

A matrix â†’ â†’ â†’ â†’ 
          [PE][PE][PE]...
          [PE][PE][PE]...
          [PE][PE][PE]...
â†“ â†“ â†“ â†“ B matrix

Workflow:
	â€¢	A and B matrices stream into the array.
	â€¢	Each PE multiplies & accumulates locally.
	â€¢	Final C matrix collected at the bottom/right edge.

âœ… The TPU essentially performs entire neural network layers in hardware.

ğŸ”© 7. Example Calculation (Numeric)

Compute:

C = A Ã— B
A = \begin{bmatrix}1 & 2\\3 & 4\end{bmatrix}, \;
B = \begin{bmatrix}5 & 6\\7 & 8\end{bmatrix}

PE(0,0) computes C_{0,0} = 1Ã—5 + 2Ã—7 = 19
PE(0,1) computes C_{0,1} = 1Ã—6 + 2Ã—8 = 22
PE(1,0) computes C_{1,0} = 3Ã—5 + 4Ã—7 = 43
PE(1,1) computes C_{1,1} = 3Ã—6 + 4Ã—8 = 50

Each PE stores its partial sum locally and passes operands diagonally across cycles.

ğŸ”¹ (B) Mapping to Convolution (CNNs)
In CNNs, convolutions can be transformed into matrix multiplications (im2col) â†’ then executed on a systolic array directly.

So, systolic arrays accelerate:
	â€¢	Dense matrix ops
	â€¢	Convolutions
	â€¢	Attention mechanisms (transformers)

ğŸ”¹ (C) 3D Systolic Arrays
Used in modern architectures (e.g., TPU v3, Cerebras Wafer-Scale Engine) â€” extend systolic communication in depth, enabling 3D data reuse.


**** Can we use FPGA for Neural network acceleration ?  ->> Read about Microsoft Brainwave



ğŸ”¹ (D) Programmable Systolic Arrays
Recent AI accelerators use reconfigurable arrays (e.g., Eyeriss, Tesla D1) that can change dataflow direction or PE roles dynamically, combining the flexibility of GPUs with systolic efficiency.


âœ… In short:

A systolic array is a parallel architecture that performs matrix multiplications (and similar linear algebra) by streaming data through a grid of MAC units, achieving massive parallelism, high data reuse, and exceptional energy efficiency.
Itâ€™s the foundation of modern AI accelerators.


*** in a systolic array, all PEs need to be same or can be different ?
In a classic systolic array, all processing elements (PEs) are typically identical (homogeneous).
This uniformity keeps the hardware structure regular, scalable, and highly efficient, which is the main reason systolic arrays are popular for dense linear algebra (e.g., matrix multiplication, convolution).
âœ… Can PEs be different?
Yes, they can be heterogeneous, but this becomes more like a generalized dataflow architecture rather than a classical systolic array.

| Property        | Homogeneous PEs | Heterogeneous PEs     |
| --------------- | --------------- | --------------------- |
| Typical?        | âœ… Yes           | âŒ Rare                |
| Dataflow        | Simple, regular | Complex               |
| Control         | Simple          | Harder                |
| VLSI efficiency | High            | Lower                 |
| Use case        | Matrix ops, CNN | Specialized pipelines |

Short answer:
They are usually the same. They can be different, but thatâ€™s no longer a traditional systolic array.
If you want, I can illustrate with examples (e.g., TPU, CNN systolic array, heterogeneous variants).

| Feature         | Homogeneous (Typical) | Heterogeneous (Possible but rare) |
| --------------- | --------------------- | --------------------------------- |
| PE type         | Same                  | Different                         |
| Regularity      | High                  | Low                               |
| Routing         | Simple                | Complicated                       |
| VLSI efficiency | Excellent             | Worse                             |
| Common?         | âœ… Yes                 | âŒ Rare                            |
| Purpose         | Matrix ops            | Mixed pipelines                   |

Real world examples:

| Architecture        | PE Type       | Comment                                                                     |
| ------------------- | ------------- | --------------------------------------------------------------------------- |
| Google TPU          | Homogeneous   | Classic MAC systolic array                                                  |
| NVIDIA Tensor Cores | Homogeneous   | Similar dense MAC model                                                     |
| Some DSP pipelines  | Heterogeneous | Preprocessing + compute + postprocessing                                    |
| FPGA designs        | Either        | Usually homogeneous for systolic, but can do heterogeneous because flexible |

âœ… Bottom Line
Traditional systolic arrays use identical PEs.
Heterogeneous PEs are possible but uncommon and deviate from the classic definition; they become more like general dataflow pipelines.


******so can I ask interviewer that if we need heterogenous kind of architecture we can choose SIMD architecture instead of systolic array ?
âœ… Best answer
Classic systolic arrays assume homogeneous PEs performing regular computations like MAC operations. If the application requires heterogeneous or irregular computation patterns, it may be better to use another model, such as MIMD/dataflow architectures, or SIMD if the operations are still uniform but more flexible.
So:
If all PEs must do the same but varied operations over time â†’ SIMD
If PEs must do different operations simultaneously â†’ MIMD

âœ… How to phrase to an interviewer
â€œTraditional systolic arrays are designed around identical PEs and very regular dataflow. If the computation becomes heterogeneous across nodes, then itâ€™s no longer a good fit as a pure systolic array.
| Architecture    | PE type       | Good for                  |
| --------------- | ------------- | ------------------------- |
| Systolic array  | Homogeneous   | Matrix ops, convolutions  |
| SIMD            | Homogeneous   | Vectors, uniform ops      |
| MIMD / Dataflow | Heterogeneous | Irregular / different ops |

Depending on the workload, a SIMD or MIMD/dataflow architecture might be more appropriate because they naturally support flexible instruction execution.â€


************1 more query, in a 2 D systolic array, we have the control to design where our data flow post calculation. right ? Whether it'll go down or go right, also whether it'll calculate and add previous value to it or send everything down or right side
Short answer:
âœ… Yes â€” in a 2-D systolic array, you (the hardware designer) control dataflow topology.
You can decide:
where the result/value is forwarded (â†’down, â†’right, etc.)
whether the PE accumulates the incoming value
whether it only forwards raw data
But this is fixed at design-time (hardware), not chosen dynamically at runtime.

âœ… More details
In a 2-D systolic array, each Processing Element (PE) typically has:
Local compute unit (e.g., MAC)
Local register/state
Fixed dataflow connections to neighbors (e.g., north â†’ south, west â†’ east)
When designing the systolic array you can choose:
Where the inputs come from (left/top)
Where outputs go (right/down)
Whether PEs store & accumulate partial results
Whether PEs only forward data without modification
So dataflow directions and accumulation behavior are defined structurally in RTL/architecture.
Example choices:
| Behavior                          | Allowed?           |
| --------------------------------- | ------------------ |
| Compute + accumulate local result | âœ…                  |
| Compute + forward result          | âœ…                  |
| Forward raw input only            | âœ…                  |
| Forward result to right only      | âœ…                  |
| Forward result downward only      | âœ…                  |
| Forward to both right & down      | âœ… (if designed so) |


âœ… But: typically standardized patterns are used

For matrix multiplication systolic arrays:

| A matrix | flows â†’ right |
| B matrix | flows â†“ down |
| Partial sums | stored/accumulated in each PE or moved diagonally |

So typical flows are:
A â†’ â†’ â†’
â†“   â†“   â†“
PE  PE  PE
PE  PE  PE
PE  PE  PE
â†“   â†“   â†“
But this is just one example.
Other applications (FIR filters, convolutions, etc.) use different flow patterns.

âœ… Can PEs send data both right and down?
Yes â€” if designed so.
Some systolic MAC arrays pass:
data A â†’ right
data B â†’ down
partial sum â†’ diagonally
Example: matrix multiply systolic array


| Question                               | Answer                 |
| -------------------------------------- | ---------------------- |
| Can designer choose routing direction? | âœ… Yes                  |
| Is routing dynamic at runtime?         | âŒ No                   |
| Can PE accumulate + forward?           | âœ… Yes                  |
| Can PE only forward?                   | âœ… Yes                  |
| Can PE send to different neighbors?    | âœ… Yes (if designed so) |

**Note For HIgh Performance each PE is also pipelined
with this pipeline, do you mean that each PE has pipelined MAC ?
Short answer:
âœ… Yes â€” in most high-performance systolic arrays, each PE contains a pipelined MAC.

âœ… Why each PE is pipelined
A PEâ€™s core operation is:
product = a Ã— b
psum    = product + psum_in

The multiply is the slowest part, so without pipelining, you would need a long clock period â†’ slow design.
By inserting pipeline registers inside the PE, you shorten the critical path and increase the operating frequency.
Because systolic arrays have large numbers of PEs, each PE must meet timing.
So pipelining must be inside every PE.
Thus:

If the MAC is pipelined, each PE contains its own pipeline stages.


âœ… Concept diagram
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
a --->â”‚   MUL       â”‚---> â”‚  ADD         â”‚ ---> psum_out
b --->â”‚ (Stage-1)   â”‚     â”‚ (Stage-2)    â”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     product_reg           psum_reg

Each PE has:


A multiplier


Adder


Pipeline registers


So every PE processes data in a pipelined fashion.

âœ… Why not pipeline outside the PEs?
Because systolic data is wave-propagating, every PE must:


Receive inputs every cycle


Produce output every cycle


So it must be internally pipelined.
External pipelining would disrupt data alignment across the array.

âœ… What does this mean physically?
If you have an N Ã— N systolic array:
N Ã— N PEs
Each PE has a pipelined MAC
Total MACs = NÂ²

Each MAC contributes pipeline latency, but the array remains fully utilized because throughput is 1 per cycle after the initial fill.

âœ… Timing Example
Suppose each PE has a 2-stage pipeline:
CyclePE OperationNMultiplyN+1Add + Forward Output
Now imagine a 2D array:
 PE(0,0) â†’ PE(0,1) â†’ PE(0,2)
    â†“         â†“         â†“
 PE(1,0) â†’ PE(1,1) â†’ PE(1,2)
    â†“         â†“         â†“
 PE(2,0) â†’ PE(2,1) â†’ PE(2,2)

After initial latency, every cycle contains useful work in every PE.

âœ… Important Clarification
âœ” Pipeline depth is local to each PE
âŒ It is NOT a shared pipeline across PEs
So if each PE is 2-stage, and you have 3 PEs in a path, the end-to-end latency is ~6 cycles (plus dataflow delays).
But throughput is still 1 result per cycle after pipeline fill.

âœ… So the answer is:

Yes, each PE contains its own pipelined MAC so that the entire systolic array can operate at high frequency and maintain full throughput.

You can tell an interviewer:

â€œEach PE typically contains a pipelined MAC. The pipeline stages are local to the PE, enabling the systolic array to run at a high clock rate and maintain 1 result per cycle throughput once the array is full.â€

