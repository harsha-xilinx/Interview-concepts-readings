++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                                                                                      Multi processor
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Read this and ARMV7 printout Chapter 14 Multicore processors
Multi-Processor
---------------
A multiprocessor system is a computer system that has two or more processors (CPUs) sharing the same physical memory and connected through a common interconnect or bus.
In simpler terms:
Multiple CPUs working together within a single system to execute tasks faster, more efficiently, or in parallel.

üß† Why Use Multiprocessors?
	‚Ä¢	Performance: Multiple CPUs can execute different parts of a program or multiple programs at once (parallelism).
	‚Ä¢	Scalability: System performance scales with added processors.
	‚Ä¢	Reliability: Some designs provide fault tolerance ‚Äî if one processor fails, others continue.
	‚Ä¢	Specialization: Certain processors can be optimized for specific tasks (e.g., graphics, signal processing)

‚öôÔ∏è 1. Architecture Types of Multiprocessor Systems

A. Shared Memory Multiprocessors (Tightly Coupled)
All processors share a common main memory.
Key traits:
	‚Ä¢	All CPUs have access to the same memory space.
	‚Ä¢	Communication occurs through shared variables.
	‚Ä¢	Synchronization is achieved using locks or semaphores.

Examples:
	‚Ä¢	Modern multicore CPUs (Intel, AMD, ARM big.LITTLE)
	‚Ä¢	SMP (Symmetric Multiprocessing) servers

B. Distributed Memory Multiprocessors (Loosely Coupled)
Each processor has its own private memory, and processors communicate via a network (message passing).
Examples:
	‚Ä¢	Cluster computing (Beowulf, HPC systems)
	‚Ä¢	Supercomputers using MPI (Message Passing Interface)



‚öôÔ∏è 3. Classification of Multiprocessor Systems

There are two primary architectures:

A. Symmetric Multiprocessing (SMP)

All processors are identical and share the same main memory and I/O.

The operating system treats all processors equally.

Suitable for general-purpose computing.

         +-----------------------------+
         |          Main Memory        |
         +-----------------------------+
             ‚Üë     ‚Üë     ‚Üë
           CPU1  CPU2  CPU3
           (Shared Bus Interconnect)

Examples:
Intel Xeon-based servers
AMD EPYC systems
Modern desktop multicore CPUs (quad-core, octa-core)
Pros:
Easy to design and program
Balanced load among processors
Cons:
Bus contention as processor count grows
Limited scalability

B. Asymmetric Multiprocessing (AMP)
Each processor is assigned a specific task.
One master processor controls the system, while slave processors perform specific functions.

Example:
Embedded systems: one CPU runs OS, another handles DSP tasks.
ARM big.LITTLE architecture (heterogeneous AMP).

Pros:
Simpler control structure
Efficient for specialized tasks

Cons:
Master becomes bottleneck
Poor load balancing

Type                                                Description                                                    Example
Asymmetric Multiprocessing (AMP)           One master CPU controls others                                   Older embedded systems
Symmetric Multiprocessing (SMP)           All CPUs are peers; share memory equally                          Modern x86/ARM servers
Massively Parallel Processing (MPP)       Many independent CPUs, each with its own memory                   Supercomputers, GPUs
Heterogeneous Multiprocessing (HMP)      Different types of cores (e.g., performance + efficiency)          ARM big.LITTLE, SoCs

‚Üí SMP and AMP systems fall under MIMD.

üß† 4. Memory Consistency and Coherence

Shared-memory multiprocessors face coherence challenges ‚Äî multiple CPUs caching the same data must stay synchronized.
	‚Ä¢	Cache Coherence Protocols (e.g., MESI, MOESI)
	‚Ä¢	Ensure all processors see consistent values.
	‚Ä¢	Memory Consistency Models
	‚Ä¢	Define when updates to shared memory become visible (e.g., sequential, relaxed).

üßæ 5. Software Support
	‚Ä¢	Operating Systems must handle:
	‚Ä¢	CPU scheduling across cores
	‚Ä¢	Synchronization primitives (mutex, semaphores)
	‚Ä¢	Interrupt handling per CPU
	‚Ä¢	Programming Models:
	‚Ä¢	Shared memory: OpenMP, pthreads
	‚Ä¢	Distributed memory: MPI
	‚Ä¢	Hybrid: CUDA (for GPUs), OpenCL



Read all these from ChatGPT before interview:
---------------------------------------------------------------------------------------------------------
Design issues in Tightly coupled Micro processor
Programming issues in Tightly coupled Micro processor
Flynn's Taxonomy
Cache Coherence Protocols:
Hardware based Multithreading

üß∞ 11. Design Perspective for Digital Design Engineers
A digital designer may be asked about:
| Topic                               | Interview Focus                                                 |
| ----------------------------------- | --------------------------------------------------------------- |
| **Bus Arbitration**                 | How multiple processors request shared bus access               |
| **Cache Coherence Hardware Design** | Implementing MESI protocol                                      |
| **Synchronization Primitives**      | Atomic instructions at hardware level                           |
| **Interconnect Design**             | Crossbar, NoC (Network on Chip)                                 |
| **Performance Metrics**             | Speedup, efficiency, Amdahl‚Äôs Law                               |
| **Power & Thermal Management**      | Core clock gating, DVFS (Dynamic Voltage and Frequency Scaling) |

üéØ 14. Common Interview Questions

What‚Äôs the difference between SMP and AMP?
Explain cache coherence and MESI protocol.
How do you maintain synchronization in shared memory systems?
Why does increasing cores not always improve performance?
Explain NUMA and its benefits.
What is the difference between UMA and NUMA architectures?
How is inter-processor communication implemented in hardware?
Describe how a bus arbitration circuit works.
What is Amdahl‚Äôs Law and its implication in multiprocessor design?
How does MESI prevent stale data in cache coherence?
üëâ Be ready to discuss bus arbitration logic and how to prevent starvation or priority inversion.
üëâ Know MESI states and how ‚Äúwrite-invalidate‚Äù and ‚Äúwrite-update‚Äù work.

In a multicore processor, a single program can be divided into multiple threads or tasks that run in parallel on multiple cores.
üîç Detailed Explanation
Programs vs. Threads
A program is made up of many instructions.
It can create multiple threads (smaller independent sequences of instructions).
Each thread can run on a different core ‚Äî that‚Äôs how one program can utilize multiple cores.
Example:
A video editing software may have separate threads for UI, encoding, decoding, and file I/O.
These threads can execute concurrently on different cores.
When It‚Äôs Not True
If the program is single-threaded, it will only run on one core at a time, even if more cores exist.
The OS might move it between cores for scheduling, but that doesn‚Äôt mean it‚Äôs using them simultaneously.
When It Is True
Programs written with multithreading or parallel processing (e.g., OpenMP, pthreads, C++ std::thread) can actually divide their work across multiple cores.
üß† In short
‚ùå A single instruction cannot be divided across cores.
‚úÖ A single program can be divided into multiple threads that run on multiple cores ‚Äî if it‚Äôs written to do so.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Flynn's Taxonomy
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

üîµ 1. SISD ‚Äî Single Instruction, Single Data
       Instruction Stream
               ‚îÇ
               ‚ñº
           +--------+
           |  CPU   |
           +--------+
               ‚îÇ
               ‚ñº
           Data Stream

Meaning: One CPU ‚Üí one instruction at a time ‚Üí one data item.

üü¢ 2. SIMD ‚Äî Single Instruction, Multiple Data
                   +----------------+
Instruction -----> | Control Unit   |
                   +----------------+
                      ‚îÇ    ‚îÇ    ‚îÇ
                      ‚ñº    ‚ñº    ‚ñº
                +------+ +------+ +------+
                | PE1  | | PE2  | | PE3  |
                +------+ +------+ +------+
                  ‚îÇ        ‚îÇ        ‚îÇ
                  ‚ñº        ‚ñº        ‚ñº
                Data1    Data2    Data3

Meaning: One instruction broadcast ‚Üí many processing elements ‚Üí operate on different data.

üü° 3. MISD ‚Äî Multiple Instruction, Single Data
Instr1 ---> +--------+
            |  PE1   |
            +--------+
                 ‚îÇ
Instr2 ---> +--------+
            |  PE2   |
            +--------+
                 ‚îÇ
Instr3 ---> +--------+
            |  PE3   |
            +--------+
                 ‚îÇ
                 ‚ñº
              Data (one stream)

Meaning: Many instruction streams operate on the same data.
Rare architecture, mostly theoretical or used for fault tolerance.

üî¥ 4. MIMD ‚Äî Multiple Instruction, Multiple Data
Instr1 ---> +--------+ ---> Data1
            | CPU1   |
            +--------+

Instr2 ---> +--------+ ---> Data2
            | CPU2   |
            +--------+

Instr3 ---> +--------+ ---> Data3
            | CPU3   |
            +--------+

Meaning: Each processor has its own instruction & data stream ‚Üí true parallelism.

üìô Flynn‚Äôs Taxonomy ‚Äì Short Notes for Quick Revision
Perfect for interviews, exams, and rapid recall.

‚≠ê SISD (Single Instruction, Single Data)
Traditional uniprocessor systems
Executes one instruction on one data item at a time
No parallelism
Examples: 8051, old single-core CPUs

‚≠ê SIMD (Single Instruction, Multiple Data)
Same instruction executed across multiple data elements
Supports data-level parallelism
Ideal for vector operations, AI, ML, graphics
Examples: GPUs, vector processors, SSE/AVX

‚≠ê MISD (Multiple Instruction, Single Data)
Many instructions operate on the same data
Very rare in practice
Used in fault-tolerant or redundant systems
Examples: Space systems with redundant processors

‚≠ê MIMD (Multiple Instruction, Multiple Data)
Each processor fetches its own instructions and data
Task-level parallelism
Most modern processors use this
Examples: Multicore CPUs, clusters, cloud servers

üß† 8. Instruction vs Thread vs Data Parallelism
| Type    | Level             | Example                  | Hardware Support     |
| ------- | ----------------- | ------------------------ | -------------------- |
| **ILP** | Within one thread | Out-of-order, pipelining | Superscalar design   |
| **TLP** | Across threads    | Multi-threaded programs  | Multicore processors |
| **DLP** | Across data       | SIMD / Vector ops        | AVX, NEON, GPU       |


************************************************************************************************************************************
Frequency and Multicore processor
************************************************************************************************************************************
The Single-Core Era (Frequency Scaling)

Initially, CPUs improved performance by raising clock frequency:

From ~100 MHz (1990s) ‚Üí 3‚Äì4 GHz (2000s).

Each generation improved pipeline depth, logic speed, and voltage scaling.

Then we hit the power wall:

P=v^2 * f;

Heat and leakage rose sharply beyond ~4‚Äì5 GHz.

Designers couldn‚Äôt keep increasing clock without overheating or violating power limits.

That‚Äôs when the shift to multicore began.

The Multicore Shift: Parallelism over Frequency
What Changed:

Instead of one core running faster, designers put more cores running slower.

Why:

Same total power budget (TDP)

Each core runs at a lower frequency (reduces power)

More cores ‚Üí more parallel work ‚Üí higher overall throughput

Throughput Effect: Total Work per Unit Time
Even if each core runs slower, adding cores usually increases total throughput, assuming the workload can use them.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
In an SMP (Symmetric Multi-Processing) system, task allocation to cores is handled almost entirely by the operating system scheduler, not by the hardware.
The hardware simply provides multiple identical cores ‚Äî the OS decides ‚Äúwho runs where.‚Äù

‚úÖ How Tasks Are Allocated to Cores in an SMP System
1Ô∏è‚É£ OS Scheduler Picks a Core

In SMP, the OS sees all CPU cores as equal and chooses where a process (task/thread) should run.
Schedulers used:
Linux ‚Üí CFS (Completely Fair Scheduler)
Windows ‚Üí Dynamic priority-based round-robin
Real-time OS ‚Üí Fixed priority / rate-monotonic / EDF
The OS decides based on:
Core availability (idle cores preferred)
Load balancing (avoid overloading one core)
CPU affinity (keep tasks on the same core if beneficial)
Priority of tasks

‚úÖ 2Ô∏è‚É£ Scheduling Domains & Load Balancing
OS divides cores into scheduling groups and periodically tries to ensure:
No core is overloaded
No core is idle when work exists elsewhere
Load balancing factors:
Number of runnable tasks per core
CPU utilization per core
Migration cost (moving task to another core)

‚úÖ 3Ô∏è‚É£ CPU Affinity (Huge Factor)
Schedulers prefer to keep a task on the same core it previously ran on.
Why?
Cache locality (warm L1/L2 cache)
Reducing migration overhead
Better performance for multi-threaded apps
So the OS tries:
Run task on its last-used core (CPU affinity)
If that core is busy ‚Üí pick an idle core
If none ‚Üí pick the least-loaded core

‚úÖ 4Ô∏è‚É£ Task Priority and Policy
Task assignment also depends on priority and scheduling policy:
Examples:
SCHED_FIFO / SCHED_RR (real-time tasks) ‚Üí run ASAP, preempt other tasks
SCHED_NORMAL (CFS) ‚Üí fairness based on virtual runtime
Fair-share policies ‚Üí assign CPU proportional to group entitlement

‚úÖ 5Ô∏è‚É£ Multi-threaded Program Behavior
For multi-threaded applications (e.g., using pthreads, OpenMP):
Threads run independently on cores
OS tries to spread them across cores for better parallelism
But may pack threads to reduce power usage (energy-aware scheduling)

üìå Summary: What Is the Basis for Distribution?
A task is allocated to a core based on:
Availability of cores (idle first)
Load balancing (least-loaded core)
CPU affinity (keep on same core if possible)
Priority / scheduling policy
Cache considerations & migration cost
Power/performance goal
Real-time constraints, if any

üß† Interview-Ready Explanation (Short Answer)
In an SMP system, tasks are assigned to cores by the OS scheduler. All cores are identical, so the OS uses factors like core load, task priority, CPU affinity, and migration cost to place tasks. Tasks typically run on their previous core to preserve cache locality, unless load balancing requires migration. Thus, task distribution is dynamic and driven primarily by software‚Äînot the hardware.

üß© Diagram: SMP Task Scheduling (Conceptual)
                +----------------+
                |    OS Kernel   |
                |  (Scheduler)   |
                +----------------+
                         |
                         v
        ------------------------------------------------
        |         Ready Queue / Run Queue             |
        |  Task A | Task B | Task C | Task D | ...    |
        ------------------------------------------------
              |           |           |           |
              |           |           |           |
     ----------------------------------------------------------------
     | Core 0        | Core 1        | Core 2        | Core 3       |
     | (CPU0)        | (CPU1)        | (CPU2)        | (CPU3)       |
     ----------------------------------------------------------------
        |               |              |              |
   Run Task A     Run Task C      Run Task B     Run Task D

üß© Diagram: How OS Chooses a Core
                    +-----------------------------+
                    |   New runnable task arrives |
                    +-------------+---------------+
                                  |
                                  v
                    +-----------------------------+
                    | Check last running CPU      |
                    | (CPU affinity)              |
                    +-------------+---------------+
                                  | Yes: idle? 
                    +-------------v----------+    
                    | Run on same core       |
                    +-------------------------+
                                  |
                               No (busy)
                                  v
                    +-----------------------------+
                    | Find least-loaded core      |
                    | (Load balancing)            |
                    +-------------+---------------+
                                  |
                                  v
                    +-----------------------------+
                    | Assign task to that core    |
                    +-----------------------------+

is SMP scheduling different from how OS choses a core ?
No ‚Äî they are the same thing.
| Concept            | Meaning                                                                      |
| ------------------ | ---------------------------------------------------------------------------- |
| **OS Scheduling**  | Choosing which task runs next (priority, policy)                             |
| **Core Selection** | Deciding which CPU core executes the chosen task                             |
| **SMP Scheduling** | OS scheduling adapted for symmetric multi-core systems ‚Üí includes both steps |

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                                                                             CPU SS INTERVIEW QUESTION
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1Ô∏è‚É£ What is SMP scheduling?
2Ô∏è‚É£ On what basis does the OS assign tasks to cores in SMP?
3Ô∏è‚É£ What is CPU affinity and why is it important?
4Ô∏è‚É£ Why does the scheduler sometimes migrate a task to another core?
5Ô∏è‚É£ What is the difference between SMP and AMP scheduling?
7Ô∏è‚É£ What happens when many threads of a program wake up at the same time?
9Ô∏è‚É£ What hardware features help SMP scheduling?
üîü How does task scheduling affect hardware like caches and interconnects?

üîπ Spend 90% focus on SMP topics:
MESI, MOESI
Directory-based coherency
Multi-core fabric interactions
Memory ordering
Scheduling & load balancing basics
Cache migration cost
Multi-core pipeline interactions
CPU affinity, task migration, NUMA basics
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                                                                                   AMP SYSTEMS
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Read from that ARM V7 print out.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                                                                                    HMP SYSTEMS
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
üî• HMP (Heterogeneous Multi-Processing)
‚≠ê Definition (Simple & Accurate):

HMP is a system where multiple cores are not identical, but the OS still sees them as a unified CPU cluster and can schedule tasks across them dynamically.

So in HMP:

Cores = heterogeneous (different performance/power)

OS = one OS running across all of them

Scheduling = OS decides which task runs on which type of core

üß© HMP sits between SMP and AMP
System	Cores	OS	Scheduling
SMP	Identical	One OS	Any core runs any task
AMP	Often different	Multiple OSs	No unified scheduling
HMP	Different	One OS	Task can run on any core (OS aware of heterogeneity)

HMP combines:

SMP‚Äôs shared-memory, unified OS

AMP‚Äôs heterogeneous cores

üß† The Most Famous HMP Example: ARM big.LITTLE

Arm‚Äôs big.LITTLE architecture is the classic HMP system.

Two types of cores:

big cores ‚Üí High performance, high power

LITTLE cores ‚Üí Energy-efficient, slower

The OS scheduler decides:

Big core for heavy tasks

LITTLE core for background or low load

More modern designs use:

big + middle + LITTLE

performance + efficiency cores (like Apple M-series, Intel Alder Lake/P/E cores)

üîß How HMP Works Internally

All cores share:

One memory subsystem

One coherency domain

One OS

Unified physical address space

Shared interconnect/fabric

But cores differ in:

Microarchitecture (issue width, cache size, pipeline depth)

Power/performance targets

Clock/voltage domains

Dynamic frequency scaling (DVFS)

üî• Scheduling in HMP

This is the biggest conceptual point an interviewer may test.

OS Scheduling Goals:

Put light tasks on LITTLE cores

Put compute-heavy tasks on big cores

Migrate tasks dynamically based on load, temperature, power policies

Many schedulers use:

Energy Aware Scheduling (EAS)

Performance governors

Thermal governors

‚öô Hardware Requirements for HMP

To support the OS treating heterogeneous cores as a unified system:

Shared coherent memory hierarchy
(L1 private, L2/L3 shared, snoop-based or directory)

ISA compatibility across core types
(ARM: big and little share same ISA subset)

Unified interrupt controller

Unified timers

Unified interconnect fabric (CCI, CMN, NoC)

Well-defined frequency/voltage domains

üìå HMP vs SMP vs AMP ‚Äî one simple diagram
SMP: [Core0][Core1][Core2][Core3]  (all identical, shared memory, one OS)

AMP: [CoreA runs RTOS] | [CoreB runs Linux] | [DSP runs firmware]
     (different OSes, separate or partially shared memory)

HMP: [big cores] [little cores] (different power/perf but one OS, shared memory)

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
üé§ Common Interview Questions on HMP
1. What is the difference between HMP and SMP?
2. How is scheduling handled in HMP?
3. Why is ISA compatibility required in HMP?
4. Does HMP require hardware coherency?
5. How does big.LITTLE migration work?
6. What is Energy-Aware Scheduling (EAS)?
7. What‚Äôs the difference between cluster migration and core migration in big.LITTLE?
8. What fabric/interconnect features must support HMP?
9. Which coherency protocols are used in heterogeneous clusters?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

